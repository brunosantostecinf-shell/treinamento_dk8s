### Criando nosso primeiro alerta

Agora que jÃ¡ temos o nosso Kube-Prometheus instalado, vamos configurar o Prometheus para monitorar o nosso cluster EKS. Para isso, vamos utilizar o kubectl port-forward para acessar o Prometheus localmente. Para isso, basta executar o seguinte comando:

```
kubectl port-forward -n monitoring svc/prometheus-k8s 39090:9090
```

Se vocÃª quiser acessar o Alertmanager, basta executar o seguinte comando:

```
kubectl port-forward -n monitoring svc/alertmanager-main 39093:9093
```

Pronto, agora vocÃª jÃ¡ sabe como que faz para acessar o Prometheus, AlertManager e o Grafana localmente. ğŸ˜„

Lembrando que vocÃª pode acessar o Prometheus e o AlertManager atravÃ©s do seu navegador, basta acessar as seguintes URLs:

- Prometheus: http://localhost:39090
- AlertManager: http://localhost:39093

Simples assim!

Evidentemente, vocÃª pode expor esses serviÃ§os para a internet ou para um VPC privado, mas isso Ã© assunto para vocÃª discutir com seu time.

Antes sair definindo um novo alerta, precisamos entender como faze-lo, uma vez que nÃ³s nÃ£o temos mais o arquivo de alertas, igual tÃ­nhamos quando instalamos o Prometheus em nosso servidor Linux.

Agora, precisamos entender que boa parte da configuraÃ§Ã£o do Prometheus estÃ¡ dentro de configmaps, que sÃ£o recursos do Kubernetes que armazenam dados em formato de chave e valor e sÃ£o muito utilizados para armazenar configuraÃ§Ãµes de aplicaÃ§Ãµes.

Para listar os configmaps do nosso cluster, basta executar o seguinte comando:

```
kubectl get configmaps -n monitoring
```

O resultado do comando acima deverÃ¡ ser parecido com o seguinte:

```
NAME                                                  DATA   AGE
adapter-config                                        1      7m20s
blackbox-exporter-configuration                       1      7m49s
grafana-dashboard-alertmanager-overview               1      7m46s
grafana-dashboard-apiserver                           1      7m46s
grafana-dashboard-cluster-total                       1      7m46s
grafana-dashboard-controller-manager                  1      7m45s
grafana-dashboard-grafana-overview                    1      7m44s
grafana-dashboard-k8s-resources-cluster               1      7m44s
grafana-dashboard-k8s-resources-namespace             1      7m44s
grafana-dashboard-k8s-resources-node                  1      7m43s
grafana-dashboard-k8s-resources-pod                   1      7m42s
grafana-dashboard-k8s-resources-workload              1      7m42s
grafana-dashboard-k8s-resources-workloads-namespace   1      7m41s
grafana-dashboard-kubelet                             1      7m41s
grafana-dashboard-namespace-by-pod                    1      7m41s
grafana-dashboard-namespace-by-workload               1      7m40s
grafana-dashboard-node-cluster-rsrc-use               1      7m40s
grafana-dashboard-node-rsrc-use                       1      7m39s
grafana-dashboard-nodes                               1      7m39s
grafana-dashboard-nodes-darwin                        1      7m39s
grafana-dashboard-persistentvolumesusage              1      7m38s
grafana-dashboard-pod-total                           1      7m38s
grafana-dashboard-prometheus                          1      7m37s
grafana-dashboard-prometheus-remote-write             1      7m37s
grafana-dashboard-proxy                               1      7m37s
grafana-dashboard-scheduler                           1      7m36s
grafana-dashboard-workload-total                      1      7m36s
grafana-dashboards                                    1      7m35s
kube-root-ca.crt                                      1      11m
prometheus-k8s-rulefiles-0                            8      7m10s
```

Como vocÃª pode ver, temos diversos configmaps que contÃ©m configuraÃ§Ãµes do Prometheus, AlertManager e do Grafana. Vamos focar no configmap prometheus-k8s-rulefiles-0, que Ã© o configmap que contÃ©m os alertas do Prometheus.

Para visualizar o conteÃºdo do configmap, basta executar o seguinte comando:

```
kubectl get configmap prometheus-k8s-rulefiles-0 -n monitoring -o yaml
```

Eu nÃ£o vou colar a saÃ­da inteira aqui porque ela Ã© enorme, mas vou colar um pedaÃ§o com um exemplo de alerta:

```
- alert: KubeMemoryOvercommit
    annotations:
        description: Cluster has overcommitted memory resource requests for Pods by
        {{ $value | humanize }} bytes and cannot tolerate node failure.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
        summary: Cluster has overcommitted memory resource requests.
    expr: |
        sum(namespace_memory:kube_pod_container_resource_requests:sum{}) - (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
        and
        (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
    for: 10m
    labels:
        severity: warning
```

Como vocÃª pode ver, o alerta acima Ã© chamado de KubeMemoryOvercommit e ele Ã© disparado quando o cluster tem mais memÃ³ria alocada para os pods do que a memÃ³ria disponÃ­vel nos nÃ³s. A sua definiÃ§Ã£o Ã© a mesma que usamos quando criamos o arquivo de alertas no nosso servidor Linux.

### Criando um novo alerta

Muito bom, jÃ¡ sabemos que temos algumas regras jÃ¡ definidas, e que elas estÃ£o dentro de um configmap. Agora, vamos criar um novo alerta para monitorar o nosso Nginx.

Mas antes, precisamos entender o que Ã© um recurso chamado PrometheusRule.

### O que Ã© um PrometheusRule?

O PrometheusRule Ã© um recurso do Kubernetes que foi instalado no momento que realizamos a instalaÃ§Ã£o dos CRDs do kube-prometheus. O PrometheusRule permite que vocÃª defina alertas para o Prometheus. Ele Ã© muito parecido com o arquivo de alertas que criamos no nosso servidor Linux, porÃ©m nesse momento vamos fazer a mesma definiÃ§Ã£o de alerta, mas usando o PrometheusRule.

Criando um PrometheusRule
Vamos criar um arquivo chamado nginx-prometheus-rule.yaml e vamos colocar o seguinte conteÃºdo:

```
apiVersion: monitoring.coreos.com/v1 # VersÃ£o da api do PrometheusRule
kind: PrometheusRule # Tipo do recurso
metadata: # Metadados do recurso (nome, namespace, labels)
  name: nginx-prometheus-rule
  namespace: monitoring
  labels: # Labels do recurso
    prometheus: k8s # Label que indica que o PrometheusRule serÃ¡ utilizado pelo Prometheus do Kubernetes
    role: alert-rules # Label que indica que o PrometheusRule contÃ©m regras de alerta
    app.kubernetes.io/name: kube-prometheus # Label que indica que o PrometheusRule faz parte do kube-prometheus
    app.kubernetes.io/part-of: kube-prometheus # Label que indica que o PrometheusRule faz parte do kube-prometheus
spec: # EspecificaÃ§Ã£o do recurso
  groups: # Lista de grupos de regras
  - name: nginx-prometheus-rule # Nome do grupo de regras
    rules: # Lista de regras
    - alert: NginxDown # Nome do alerta
      expr: up{job="nginx"} == 0 # ExpressÃ£o que serÃ¡ utilizada para disparar o alerta
      for: 1m # Tempo que a expressÃ£o deve ser verdadeira para que o alerta seja disparado
      labels: # Labels do alerta
        severity: critical # Label que indica a severidade do alerta
      annotations: # AnotaÃ§Ãµes do alerta
        summary: "Nginx is down" # TÃ­tulo do alerta
        description: "Nginx is down for more than 1 minute. Pod name: {{ $labels.pod }}" # DescriÃ§Ã£o do alerta
```

Agora, vamos criar o PrometheusRule no nosso cluster:

```
kubectl apply -f nginx-prometheus-rule.yaml
```

Agora, vamos verificar se o PrometheusRule foi criado com sucesso:

```
kubectl get prometheusrules -n monitoring
```

A saÃ­da deve ser parecida com essa:

```
NAME                              AGE
alertmanager-main-rules           92m
grafana-rules                     92m
kube-prometheus-rules             92m
kube-state-metrics-rules          92m
kubernetes-monitoring-rules       92m
nginx-prometheus-rule             20s
node-exporter-rules               91m
prometheus-k8s-prometheus-rules   91m
prometheus-operator-rules         91m
```

Agora nÃ³s jÃ¡ temos um novo alerta configurado em nosso Prometheus. Lembrando que temos a integraÃ§Ã£o com o AlertManager, entÃ£o, quando o alerta for disparado, ele serÃ¡ enviado para o AlertManager e o AlertManager vai enviar uma notificaÃ§Ã£o, por exemplo, para o nosso Slack ou e-mail.

VocÃª pode acessar o nosso alerta tanto no Prometheus quanto no AlertManager.

Vamos imaginar que vocÃª precisa criar um novo alerta para monitorar a quantidade de requisiÃ§Ãµes simultÃ¢neas que o seu Nginx estÃ¡ recebendo. Para isso, vocÃª precisa criar uma nova regra no PrometheusRule. Podemos utilizar o mesmo arquivo nginx-prometheus-rule.yaml e adicionar a nova regra no final do arquivo:

```
apiVersion: monitoring.coreos.com/v1 # VersÃ£o da api do PrometheusRule
kind: PrometheusRule # Tipo do recurso
metadata: # Metadados do recurso (nome, namespace, labels)
  name: nginx-prometheus-rule
  namespace: monitoring
  labels: # Labels do recurso
    prometheus: k8s # Label que indica que o PrometheusRule serÃ¡ utilizado pelo Prometheus do Kubernetes
    role: alert-rules # Label que indica que o PrometheusRule contÃ©m regras de alerta
    app.kubernetes.io/name: kube-prometheus # Label que indica que o PrometheusRule faz parte do kube-prometheus
    app.kubernetes.io/part-of: kube-prometheus # Label que indica que o PrometheusRule faz parte do kube-prometheus
spec: # EspecificaÃ§Ã£o do recurso
  groups: # Lista de grupos de regras
  - name: nginx-prometheus-rule # Nome do grupo de regras
    rules: # Lista de regras
    - alert: NginxDown # Nome do alerta
      expr: up{job="nginx"} == 0 # ExpressÃ£o que serÃ¡ utilizada para disparar o alerta
      for: 1m # Tempo que a expressÃ£o deve ser verdadeira para que o alerta seja disparado
      labels: # Labels do alerta
        severity: critical # Label que indica a severidade do alerta
      annotations: # AnotaÃ§Ãµes do alerta
        summary: "Nginx is down" # TÃ­tulo do alerta
        description: "Nginx is down for more than 1 minute. Pod name: {{ $labels.pod }}" # DescriÃ§Ã£o do alerta

    - alert: NginxHighRequestRate # Nome do alerta
        expr: rate(nginx_http_requests_total{job="nginx"}[5m]) > 10 # ExpressÃ£o que serÃ¡ utilizada para disparar o alerta
        for: 1m # Tempo que a expressÃ£o deve ser verdadeira para que o alerta seja disparado
        labels: # Labels do alerta
            severity: warning # Label que indica a severidade do alerta
        annotations: # AnotaÃ§Ãµes do alerta
            summary: "Nginx is receiving high request rate" # TÃ­tulo do alerta
            description: "Nginx is receiving high request rate for more than 1 minute. Pod name: {{ $labels.pod }}" # DescriÃ§Ã£o do alerta
```

Pronto, adicionamos uma nova definiÃ§Ã£o de alerta em nosso PrometheusRule. Agora vamos atualizar o nosso PrometheusRule:

```
kubectl apply -f nginx-prometheus-rule.yaml
```

Agora, vamos verificar se o PrometheusRule foi atualizado com sucesso:

```
kubectl get prometheusrules -n monitoring nginx-prometheus-rule -o yaml
```

A saÃ­da deve ser parecida com essa:

```
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"kube-prometheus","app.kubernetes.io/part-of":"kube-prometheus","prometheus":"k8s","role":"alert-rules"},"name":"nginx-prometheus-rule","namespace":"monitoring"},"spec":{"groups":[{"name":"nginx-prometheus-rule","rules":[{"alert":"NginxDown","annotations":{"description":"Nginx is down for more than 1 minute. Pod name: {{ $labels.pod }}","summary":"Nginx is down"},"expr":"up{job=\"nginx\"} == 0","for":"1m","labels":{"severity":"critical"}},{"alert":"NginxHighRequestRate","annotations":{"description":"Nginx is receiving high request rate for more than 1 minute. Pod name: {{ $labels.pod }}","summary":"Nginx is receiving high request rate"},"expr":"rate(nginx_http_requests_total{job=\"nginx\"}[5m]) \u003e 10","for":"1m","labels":{"severity":"warning"}}]}]}}
  creationTimestamp: "2023-03-01T14:14:00Z"
  generation: 2
  labels:
    app.kubernetes.io/name: kube-prometheus
    app.kubernetes.io/part-of: kube-prometheus
    prometheus: k8s
    role: alert-rules
  name: nginx-prometheus-rule
  namespace: monitoring
  resourceVersion: "24923"
  uid: c0a6914d-9a54-4083-bdf8-ebfb5c19077d
spec:
  groups:
  - name: nginx-prometheus-rule
    rules:
    - alert: NginxDown
      annotations:
        description: 'Nginx is down for more than 1 minute. Pod name: {{ $labels.pod
          }}'
        summary: Nginx is down
      expr: up{job="nginx"} == 0
      for: 1m
      labels:
        severity: critical
    - alert: NginxHighRequestRate
      annotations:
        description: 'Nginx is receiving high request rate for more than 1 minute.
          Pod name: {{ $labels.pod }}'
        summary: Nginx is receiving high request rate
      expr: rate(nginx_http_requests_total{job="nginx"}[5m]) > 10
      for: 1m
      labels:
        severity: warning
```

Pronto, o alerta foi criado com sucesso e vocÃª pode conferir no Prometheus ou no AlertManager.

Com o novo alerta, caso o Nginx esteja recebendo mais de 10 requisiÃ§Ãµes por minuto, o alerta serÃ¡ disparado e vocÃª receberÃ¡ uma notificaÃ§Ã£o no Slack ou e-mail, claro, dependendo da configuraÃ§Ã£o que vocÃª fez no AlertManager.

Acho que jÃ¡ podemos chamar o dia de hoje de sucesso absoluto, pois entendemos como funciona para criar um novo target para o Prometheus, bem como criar um novo alerta para o AlertManager/Prometheus.

Agora vocÃª precisa dar asas para a sua imaginaÃ§Ã£o e sair criando tudo que Ã© exemplo de de alerta que vocÃª bem entender, e claro, coloque mais serviÃ§os no seu cluster Kubernetes para que vocÃª possa monitorar tudo que Ã© possÃ­vel atravÃ©s do ServiceMonitor e PrometheusRule do Prometheus Operator, e claro, nÃ£o esqueÃ§a de compartilhar com a gente o que vocÃª criou.